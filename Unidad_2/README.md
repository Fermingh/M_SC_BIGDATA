
# Unidad 2: Adquisición, Ingesta y Procesamiento de Datos 📡

![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?logo=numpy&logoColor=white)
![Spark](https://img.shields.io/badge/Spark-FDEE21?logo=apachespark&logoColor=black)
![Kafka](https://img.shields.io/badge/Kafka-231F20?logo=apachekafka&logoColor=white)
![BeautifulSoup](https://img.shields.io/badge/BeautifulSoup-4B8BBE?logo=python&logoColor=white)
![Selenium](https://img.shields.io/badge/Selenium-43B02A?logo=selenium&logoColor=white)
![API](https://img.shields.io/badge/API-FF9800?logo=apachespark&logoColor=black)
![Flink](https://img.shields.io/badge/Flink-EA4335?logo=apacheflink&logoColor=white)
![Airflow](https://img.shields.io/badge/Airflow-017CEE?logo=apacheairflow&logoColor=white)

---

## ⚡ Objetivos Específicos

- Aplicar **técnicas y metodologías** para la adquisición, ingesta y procesamiento eficiente de grandes volúmenes de datos provenientes de fuentes diversas.
- Garantizar la **calidad, integridad y pertinencia** de los datos para su análisis posterior📊.
- Distinguir entre **procesamiento batch** y **procesamiento en tiempo real**🔄.
- Implementar **flujos de procesamiento distribuido** empleando herramientas especializadas para la limpieza y transformación de datos.

---

## 🏅 Competencias Específicas a Desarrollar

- Investigar y comparar **métodos de adquisición e ingesta de datos**, incluyendo técnicas de web scraping, consumo de APIs y sistemas de ingesta en tiempo real💡.
- Identificar y documentar **fuentes de datos** (estructurados, semiestructurados y no estructurados) relevantes para proyectos de Big Data.
- Desarrollar prácticas de **extracción y procesamiento** de datos utilizando herramientas como BeautifulSoup, Pandas, NumPy y Kafka🔬.
- Analizar y diferenciar los **paradigmas batch vs. streaming**, determinando casos de aplicación para cada uno.
- Aplicar técnicas de **preprocesamiento**: normalización, imputación y detección de outliers usando Python y sus principales librerías🧹.

---

## 🖥️ Tecnologías principales en la unidad

- Web scraping: **BeautifulSoup**, **Selenium**
- APIs públicas: **REST**, **Twitter API**, **Google Maps API**
- Ingesta en tiempo real: **Kafka**, **Flume**, **NiFi**, **Airflow**
- Procesamiento distribuido: **Spark**, **Flink**, **Storm**

---

## 📚 Actividades de aprendizaje recomendadas

- Investigar y comparar métodos de adquisición e ingesta de datos.
- Documentar fuentes relevantes para proyectos de Big Data.
- Practicar extracción de datos mediante web scraping y consumo de APIs públicas.
- Desarrollar ejercicios de ingesta en tiempo real con herramientas especializadas.
- Analizar diferencias entre procesamiento batch y streaming.
- Elaborar reportes técnicos sobre calidad de datos y mejores prácticas.
- Participar en casos prácticos de integración y procesamiento colaborativo.

---

## 🤝 Soft Skills y competencias genéricas reforzadas

- Análisis y síntesis de información
- Organización y planificación de proyectos
- Comunicación oral y escrita
- Trabajo colaborativo en equipos multidisciplinares
- Pensamiento crítico y resolución de problemas

---

> 🚦 **Recuerda:** Esta unidad te prepara para manejar cualquier tipo de fuente de datos y optimizar el flujo de procesamiento desde la adquisición hasta la limpieza, asegurando datos confiables para cualquier análisis avanzado.

